{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c282a6",
   "metadata": {},
   "source": [
    "# Audio Features\n",
    "\n",
    "Audio features are compact, meaningful measurements extracted from short speech frames that make learning easier.  \n",
    "\n",
    "Intuitively:  \n",
    "“What properties of this sound matter for my task?”  \n",
    "some examples:  \n",
    "\n",
    "| Task       | What matters              |\n",
    "| ---------- | ------------------------- |\n",
    "| Speaker ID | Vocal tract shape, timbre |\n",
    "| Emotion    | Pitch, energy, rhythm     |\n",
    "| Health     | Voice stability, noise    |\n",
    "| Age/Gender | Pitch range, formants     |\n",
    "\n",
    "# Time domain features (No FFT)\n",
    "\n",
    "as we have seen earlier that FFT(fast fourier transform) is used to convert time domain data to frequency domain.  \n",
    "\n",
    "## Energy (Loudness)\n",
    "\n",
    "What:  \n",
    "How strong the signal is in the frame.  \n",
    "\n",
    "Intuition:  \n",
    "Loud vs quiet speech.  \n",
    "\n",
    "Used for:\n",
    "- Voice activity detection (speech vs silence)\n",
    "- Emotion (anger vs calm)\n",
    "- Stress detection\n",
    "\n",
    "## Zero Crossing Rate (ZCR)\n",
    "\n",
    "What:  \n",
    "How often the waveform crosses zero.  \n",
    "\n",
    "Intuition:\n",
    "- Noisy sounds → high ZCR\n",
    "- Voiced sounds → low ZCR\n",
    "\n",
    "Used for:\n",
    "- Voiced vs unvoiced detection\n",
    "- Rough speech characterization\n",
    "\n",
    "Rarely used alone in modern systems.  \n",
    "\n",
    "# Frequency domain and spectral features(after FFT)\n",
    "\n",
    "## Spectral Centroid\n",
    "\n",
    "What:  \n",
    "“Center of mass” of frequencies.\n",
    "\n",
    "Intuition:  \n",
    "- Low centroid → dark / deep voice\n",
    "- High centroid → bright / sharp sound\n",
    "\n",
    "Used for:\n",
    "- Emotion\n",
    "- Timbre analysis\n",
    "\n",
    "## Spectral Bandwidth, Roll-off, Flux\n",
    "\n",
    "| Feature   | Intuition                  |\n",
    "| --------- | -------------------------- |\n",
    "| Bandwidth | How spread frequencies are |\n",
    "| Roll-off  | High-frequency cutoff      |\n",
    "| Flux      | How fast spectrum changes  |\n",
    "\n",
    "These are common in music/audio, less dominant in speech ML today.\n",
    "\n",
    "## Cepstral Features — MFCCs (Very Important)\n",
    "\n",
    "In speech processing and machine learning, Cepstral Features are the specific numbers (coefficients) extracted during Cepstrum analysis. They are essentially a \"summary\" of the sound's texture and characteristics.\n",
    "\n",
    "### The cepstrum\n",
    "\n",
    "To understand the Cepstrum, think about how you speak:\n",
    "\n",
    "- The Source (Vocal Cords): Your vocal cords vibrate at a certain speed. This creates the pitch of your voice.\n",
    "- The Filter (Vocal Tract): Your mouth and tongue change shape to turn those vibrations into specific sounds like \"Ah\" or \"Ee.\" This is the timbre or \"texture\" of the sound.\n",
    "\n",
    "When you record a sound, the Source and Filter are \"tangled\" together. The Spectrum shows them as one messy line. The Cepstrum is the mathematical tool that reaches in and pulls them apart so you can analyze the pitch and the texture separately.  \n",
    "\n",
    "In standard signal processing, the Fourier Transform (FFT) moves you from Time to Frequency. To get back, you use the Inverse Fourier Transform (IFFT).  \n",
    "\n",
    "In a Cepstrum, you do something unusual:\n",
    "1. Forward: You take the FFT (Time → Frequency).\n",
    "2. Processing: You take the Logarithm of that spectrum.\n",
    "3. \"Backwards\": You take the IFFT (Frequency → \"New Time\").\n",
    "\n",
    "Because you took the logarithm in the middle, the IFFT doesn't bring you back to the original sound wave. Instead, it lands you in a \"pseudo-time\" domain called Quefrency. It feels like you turned around and walked back toward the starting line, but ended up in a parallel dimension.  \n",
    "\n",
    "The scientists who created this (Tukey et al.) realized they were doing everything in reverse. To signal this to other engineers, they flipped the names of everything:\n",
    "- SPECtrum\t-> CEPStrum\n",
    "- FREQuency ->\tQUEFrency\n",
    "- FILTer -> LIFTer\n",
    "- PHASe -> SAPHe\n",
    "- HARMonic -> RAHMonic\n",
    "\n",
    "### Mel-Frequency Cepstral Coefficients\n",
    "\n",
    "The MFCC is actually a specific type of Cepstrum. The word \"Cepstral\" in MFCC tells you that it uses the same core \"backwards\" logic (taking the transform of a log-spectrum).\n",
    "\n",
    "The Pipeline Connection:\n",
    "- Cepstrum: Signal → FFT → Log → IFFT\n",
    "- MFCC: Signal → FFT → Mel-Filterbank → Log → DCT(descrete cosine transform - a variation of IFFT)\n",
    "\n",
    "#### Importance of the Mel Filterbank\n",
    "\n",
    "Humans are very good at telling the difference between 500 Hz and 600 Hz, but we are terrible at telling the difference between 10,000 Hz and 10,100 Hz, even though the gap (100 Hz) is the same.\n",
    "\n",
    "- The Cepstrum doesn't care about this; it analyzes everything linearly.\n",
    "- The MFCC \"warps\" the frequency axis using the Mel Scale to match our ears. This is why MFCCs are the \"gold standard\" for AI models that need to \"hear\" like people (e.g., Alexa, Siri).\n",
    "\n",
    "### Why MFCCs Exist\n",
    "\n",
    "Key problem:\n",
    "- Spectrum is too detailed\n",
    "- Humans care about spectral envelope, not fine harmonics\n",
    "\n",
    "MFCCs capture:\n",
    "- The shape of the vocal tract, not pitch harmonics\n",
    "\n",
    "### Intuition\n",
    "- Mel filters ≈ how humans hear frequency\n",
    "- Log ≈ loudness perception\n",
    "- DCT ≈ compression & decorrelation\n",
    "\n",
    "MFCCs ≈ compressed vocal tract signature\n",
    "\n",
    "### What MFCCs capture\n",
    "\n",
    "| Captures          | Ignores        |\n",
    "| ----------------- | -------------- |\n",
    "| Vocal tract shape | Exact pitch    |\n",
    "| Timbre            | Phase          |\n",
    "| Speaker identity  | Fine harmonics |\n",
    "\n",
    "- useful notebook - [mfcc tutorial](https://www.kaggle.com/code/ilyamich/mfcc-implementation-and-tutorial)\n",
    "\n",
    "## Prosodic features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20659b4d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
