{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d11c1c",
   "metadata": {},
   "source": [
    "# In this notebook we are exploring Speech emotion recognition using wav2vec2 model\n",
    "\n",
    "what we are trying to achieve is the following structure:\n",
    "\n",
    "![Model architecture](images/SERmodelArchitecture.png)\n",
    "\n",
    "- The input is raw audio or .wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a474999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from speechbrain.lobes.models.huggingface_transformers.wav2vec2 import Wav2Vec2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c4fa8",
   "metadata": {},
   "source": [
    "## Function to preprocess an load the dataset\n",
    "\n",
    "### Explanation of how stereo audio is converted to mono audio.  \n",
    "1. The Condition: if wav.shape[0] > 1:  \n",
    "\n",
    "In PyTorch, audio tensors are typically loaded in the shape [channels, samples].\n",
    "\n",
    "- wav.shape[0] refers to the number of channels.\n",
    "- If the value is 1, the audio is already Mono.\n",
    "- If the value is 2, the audio is Stereo (Left and Right channels).\n",
    "- The if statement checks if there is more than one channel present.\n",
    "\n",
    "2. The Operation: torch.mean(wav, dim=0, keepdim=True)  \n",
    "\n",
    "If the audio has multiple channels, this line merges them:\n",
    "\n",
    "- torch.mean(..., dim=0): It calculates the average value across the channel dimension. For a stereo file, it adds the Left and Right samples together and divides by 2. This is the standard way to downmix stereo to mono without clipping the audio.\n",
    "\n",
    "- keepdim=True: This ensures the tensor retains its 2D shape.\n",
    "    - Without it, a shape of [2, 44100] would become [44100].\n",
    "    - With it, the shape becomes [1, 44100]. This is important because most deep learning models expect the channel dimension to be present, even if it's just a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28243da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    \"\"\"\n",
    "    Loads an audio file and prepares it for wav2vec2\n",
    "    Returns a tensor of shape [time_samples]\n",
    "    \"\"\"\n",
    "    # Load the audio\n",
    "    wav, sr = torchaudio.load(file_path)\n",
    "\n",
    "    # resample the audio if it is not 16kHz\n",
    "    if(sr!=16000):\n",
    "        resampler = torchaudio.transforms.Resample(\n",
    "            orig_freq=sr,\n",
    "            new_freq=16000\n",
    "        )\n",
    "        wav = resampler(wav)\n",
    "    \n",
    "    # if the audio is stereo convert it into mono\n",
    "    if wav.shape[0]>1:\n",
    "        wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "\n",
    "    # remove the channel dimention\n",
    "    # since we want to remove the first dimention which is at 0th index so we provide that into the squeeze function\n",
    "    wav = wav.squeeze(0)\n",
    "\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef007ada",
   "metadata": {},
   "source": [
    "## Now we will define the function that initializes the wav2vec2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e265c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(num_emotions=2, freeze_wav2vec=True):\n",
    "    \"\"\"\n",
    "    Creates wav2vec2 encoder and classifier\n",
    "    Returns: wav2vec2_model, classifier\n",
    "    \"\"\"\n",
    "    # Load wav2vec2 from HuggingFace via SpeechBrain\n",
    "    wav2vec2 = Wav2Vec2(\n",
    "        source=\"facebook/wav2vec2-base-960h\",\n",
    "        save_path=\"./wav2vec2_checkpoints\"\n",
    "    )\n",
    "    \n",
    "    # Freeze if needed\n",
    "    if freeze_wav2vec:\n",
    "        for param in wav2vec2.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Dense layers on top\n",
    "    classifier = nn.Sequential(\n",
    "        nn.Linear(768, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(128, num_emotions)\n",
    "    )\n",
    "    \n",
    "    return wav2vec2, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9befb171",
   "metadata": {},
   "source": [
    "## Now we define the function which gives the prediction\n",
    "\n",
    "### The unsqueeze() and squeeze() what they do and their purpose\n",
    "\n",
    "let us take a concrete example:  \n",
    "\n",
    "1. Start with a simple 1D Audio Tensor: wav = [0.1, 0.2, 0.3]  \n",
    "    - Shape: (3) (3 samples)\n",
    "\n",
    "2. Apply unsqueeze(0): wav = [[0.1, 0.2, 0.3]]\n",
    "    - Shape: (1, 3) (1 row, 3 columns)\n",
    "\n",
    "Intuition: You added a new set of outer brackets. You now have a **\"List of Lists,\"** even though the outer list only contains one thing.\n",
    "\n",
    "3. Apply unsqueeze(1) instead: wav = [[0.1], [0.2], [0.3]]\n",
    "    - Shape: (3, 1) (3 rows, 1 column)\n",
    "Intuition: You put a box around every individual sample. This is like turning a horizontal line into a vertical column.  \n",
    "\n",
    "As we know that neural networks expect numbers on each of their nodes and they take 1 nuber from each column in a row that is why we need to unsqueze the data so that it becomes a matrix with 1 row and 768 colums in our case which is the size of the vector embedding that wav2vec2 gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f531c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(wav, wav2vec2, classifier):\n",
    "    '''\n",
    "    Gets the emotion prediction for a single audio file\n",
    "    wav : the raw audio waveform\n",
    "    return : logits i.e. the number corresponding to a specific emotion\n",
    "    '''\n",
    "    #Add batch dimension : [time_samples]->[1,time_samples]\n",
    "    wav = wav.unsqueeze(0)\n",
    "\n",
    "    #Get the wav2vec2 Embeddings\n",
    "    with torch.no_grad():\n",
    "        feats = wav2vec2(wav) # Shape: [1, time_frames, 768]\n",
    "    \n",
    "    # Mean pooling across time\n",
    "    embeddings = torch.mean(feats, dim=1) # Shape: [1, 768]\n",
    "\n",
    "    logits = classifier(embeddings) # Shape: [1, num_emotions]\n",
    "\n",
    "    return logits.squeeze(0) # Remove batch dim: [num_emotions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8d856",
   "metadata": {},
   "source": [
    "let us talk a bit about the output of the wav2vec2 embedding.\n",
    "\n",
    "- Why is the shape [1, time_frames, 768]?  \n",
    "    This shape represents a 3D Tensor, which is standard for sequence modeling. Each dimension has a specific meaning:\n",
    "    - `1` (Batch Size): This represents the number of audio samples you processed at once. Since you passed a single waveform, the batch size is 1.\n",
    "    - `time_frames` (Sequence Length): Audio is a continuous signal. Wav2Vec2 divides the audio into small, overlapping windows (frames).\n",
    "        - For Wav2Vec2, the CNN encoder typically produces one feature vector for every 20ms of audio.\n",
    "        - If your audio is 1 second long, you will have roughly 50 time frames.\n",
    "\n",
    "    - `768` (Embedding Dimension/Hidden Size): This is the \"depth\" of the model. For the Wav2Vec2-Base architecture, every single time frame is represented by a vector of 768 numbers. These numbers capture the phonetic and acoustic characteristics of that specific slice of time.\n",
    "\n",
    "- Why does Mean Pooling \"remove\" the time_frames?  \n",
    "    Mean pooling is a mathematical reduction. When you call torch.mean(feats, dim=1), you are telling PyTorch to collapse the second dimension (the time axis) by calculating the average.  \n",
    "    \n",
    "    The Mathematics  \n",
    "    \n",
    "    If your `feats` tensor is visualized as a matrix for each batch:  \n",
    "    \n",
    "    $X = [f_1, f_2, f_3, \\ldots, f_T]$  \n",
    "\n",
    "    Where each $f_tâ€‹$ is a vector of size 768, mean pooling performs:  \n",
    "\n",
    "    $\\text{Embedding} = \\frac{1}{T} \\sum_{t=1}^{T} f_t$  \n",
    "\n",
    "    The Result:  \n",
    "    \n",
    "    - Input: 100 different vectors (one for each moment in time).\n",
    "    - Output: 1 single vector (the \"average\" of all those moments).\n",
    "\n",
    "    The dimension dim=1 disappears because it has been aggregated. You move from a \"sequence of features\" to a \"single global feature\" that represents the entire audio clip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e3850",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa27fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(wavs, labels, wav2vec2, classifier, optimizer, criterion):\n",
    "    '''\n",
    "    Trains one batch of data\n",
    "    wavs : list of wavform tensors\n",
    "    labels : list of emotion tensors\n",
    "    '''\n",
    "    # We need to pad waveforms to same length in batch\n",
    "    max_len = max([w.shape[0] for w in wavs])\n",
    "    padded_wavs = []\n",
    "    for w in wavs:\n",
    "        if w.shape[0]<max_len:\n",
    "            padding = torch.zeroes(max_len - w.shape[0])\n",
    "            w = torch.cat([w,padding])\n",
    "        padded_wavs.append(w)\n",
    "\n",
    "    # Stack into batch : [batch_size, time_samples]\n",
    "    batch_wavs = torch.stack(padded_wavs)\n",
    "\n",
    "    #forward pass\n",
    "    feats = wav2vec2(batch_wavs) # [batch_size, time_frames, 768]\n",
    "    embeddings = torch.mean(feats, dim=1)  # [batch_size, 768]\n",
    "    logits = classifier(embeddings)  # [batch_size, num_emotions]\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(logits, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5f4e6",
   "metadata": {},
   "source": [
    "## Why do we pad the waveforms for them to be of same size?\n",
    "- most machine learning and deep learning models expect rectandgular data\n",
    "- a jagged matrix will not be accepted as a input\n",
    "\n",
    "That is why we need to pad the waveforms to make their length equal to the longest wav file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05be3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(audio_files, emotion_labels):\n",
    "    \"\"\"\n",
    "    Loads all audio files and prepares them for training\n",
    "    audio_files: list of file paths ['audio1.wav', 'audio2.wav', ...]\n",
    "    emotion_labels: list of integers [0, 1, 0, 1, ...] where 0=sad, 1=happy, etc.\n",
    "    Returns: list of waveforms, tensor of labels\n",
    "    \"\"\"\n",
    "    waveforms = []\n",
    "    for audio_path in audio_files:\n",
    "        waveform = load_audio(audio_path)\n",
    "        waveforms.append(waveform)\n",
    "    \n",
    "    # Convert labels to tensor\n",
    "    labels = torch.tensor(emotion_labels, dtype=torch.long)\n",
    "    \n",
    "    return waveforms, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64532bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(waveforms, labels, wav2vec2, classifier, num_epochs=10, batch_size=8, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "    \n",
    "        for i in range(0, len(waveforms), batch_size):\n",
    "            batch_waveforms = waveforms[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            \n",
    "            # Pad waveforms to same length\n",
    "            max_len = max([w.shape[0] for w in batch_waveforms])\n",
    "            padded = []\n",
    "            for w in batch_waveforms:\n",
    "                if w.shape[0] < max_len:\n",
    "                    padding = torch.zeros(max_len - w.shape[0])\n",
    "                    w = torch.cat([w, padding])\n",
    "                padded.append(w)\n",
    "            \n",
    "            # Stack into batch tensor\n",
    "            batch_tensor = torch.stack(padded)\n",
    "            \n",
    "            # Forward pass through wav2vec2\n",
    "            with torch.no_grad():\n",
    "                feats = wav2vec2(batch_tensor)\n",
    "            \n",
    "            # Mean pooling\n",
    "            embeddings = torch.mean(feats, dim=1)\n",
    "            \n",
    "            # Forward through classifier\n",
    "            logits = classifier(embeddings)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5f207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\anaconda3\\envs\\speechbrain_env\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.7053\n",
      "Epoch 2/20, Loss: 0.6984\n",
      "Epoch 3/20, Loss: 0.6911\n",
      "Epoch 4/20, Loss: 0.6792\n",
      "Epoch 5/20, Loss: 0.6809\n",
      "Epoch 6/20, Loss: 0.6611\n",
      "Epoch 7/20, Loss: 0.6903\n",
      "Epoch 8/20, Loss: 0.7361\n",
      "Epoch 9/20, Loss: 0.7025\n",
      "Epoch 10/20, Loss: 0.6504\n",
      "Epoch 11/20, Loss: 0.7235\n",
      "Epoch 12/20, Loss: 0.6563\n",
      "Epoch 13/20, Loss: 0.6902\n",
      "Epoch 14/20, Loss: 0.7031\n",
      "Epoch 15/20, Loss: 0.7111\n",
      "Epoch 16/20, Loss: 0.6548\n",
      "Epoch 17/20, Loss: 0.7161\n",
      "Epoch 18/20, Loss: 0.6728\n",
      "Epoch 19/20, Loss: 0.6488\n",
      "Epoch 20/20, Loss: 0.6930\n",
      "Training complete! Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your training data\n",
    "    audio_files = [\n",
    "   'D:/work/Aiml/conversational AI/speechbrain demo/archive/Actor_02/03-01-01-01-01-01-02.wav',\n",
    "   'D:/work/Aiml/conversational AI/speechbrain demo/archive/Actor_02/03-01-01-01-01-02-02.wav',\n",
    "   'D:/work/Aiml/conversational AI/speechbrain demo/archive/Actor_02/03-01-01-01-02-01-02.wav',\n",
    "   'D:/work/Aiml/conversational AI/speechbrain demo/archive/Actor_02/03-01-01-01-02-02-02.wav',\n",
    "    ]\n",
    "    # These are the labels for each audio that I provide\n",
    "    emotion_labels = [1, 0, 1, 0]  # 1=happy, 0=sad\n",
    "    \n",
    "    # Initialize models\n",
    "    wav2vec2, classifier = initialize_models(num_emotions=2, freeze_wav2vec=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    waveforms, labels = prepare_dataset(audio_files, emotion_labels)\n",
    "    \n",
    "    # Train\n",
    "    trained_classifier = train_model(\n",
    "        waveforms, \n",
    "        labels, \n",
    "        wav2vec2, \n",
    "        classifier,\n",
    "        num_epochs=20,\n",
    "        batch_size=4,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Save the trained classifier\n",
    "    torch.save(trained_classifier.state_dict(), 'emotion_classifier.pth')\n",
    "    print(\"Training complete! Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2eb31",
   "metadata": {},
   "source": [
    "Some useful terms:\n",
    "- Batch size - batch_size is the number of training examples processed before the model's weights are updated. One update = one batch (one iteration).\n",
    "- epoch - One epoch = one full pass over the dataset.\n",
    "- Iterations per epoch = ceil(dataset_size / batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechbrain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
