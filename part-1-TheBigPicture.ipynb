{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9a5699",
   "metadata": {},
   "source": [
    "# The big picture\n",
    "\n",
    "![flowchart](images/The%20Big%20Picture.png)\n",
    "\n",
    "## What is raw audio?\n",
    "\n",
    "### waveform(Time domain signal)\n",
    "\n",
    "- waveform and raw audio are not exactly the same things but in speech ML tasks they are used interchangeably.\n",
    "\n",
    "waveform or raw audio is a 1-D time series.  \n",
    "each value = air pressure deviation at a moment in time.  \n",
    "(for understanding only think of it as a really long 1-D array, but it is not an array it is a tensor)\n",
    "\n",
    "### sampling rate\n",
    "\n",
    "We need to convert the natural signals into digital ones since the natural waveform has infinite points as it is represented by a line.  \n",
    "So to represent it in computers feasibly we take discrete values from the natural waveform.  \n",
    "Now at the rate that we take these **discrete values** or **samples** is called the sampling rate.  \n",
    "\n",
    "Each value in that tensor we talked about earlier is 1 sample.  \n",
    "\n",
    "| Sampling Rate | Meaning                 |\n",
    "| ------------- | ----------------------- |\n",
    "| 16 kHz        | 16,000 samples / second |\n",
    "| 44.1 kHz      | CD quality              |\n",
    "| 48 kHz        | Professional audio      |\n",
    "\n",
    "Speech ML standard: 16 kHz\n",
    "\n",
    "Why?\n",
    "- Human speech mostly lives below 8 kHz\n",
    "- Nyquist rule: sample ≥ 2× max frequency\n",
    "\n",
    "- speechbrain assumes 16 kHz sampling rate\n",
    "\n",
    "## Framing & Windowing\n",
    "\n",
    "The main question arises why do we need framing?\n",
    "\n",
    "The problem we encounter while processing speech is that it **quasi-stationary**.  \n",
    "what that means is that over short intervals (~20-30 ms), speech properties are stable, but over long durations they change.\n",
    "\n",
    "So to solve this problem we frame the signal.\n",
    "\n",
    "### Framing\n",
    "\n",
    "framing means splitting the waveform in short chunks.\n",
    "\n",
    "| Parameter\t        | Typical Value |\n",
    "| ----------------- | ------------- |\n",
    "| Frame length      |\t25 ms       |\n",
    "| Frame shift (hop) |\t10 ms       |\n",
    "\n",
    "At 16 kHz:  \n",
    "25 ms → 400 samples  \n",
    "10 ms → 160 samples  \n",
    "\n",
    "Waveform:  |-----------------------------|  \n",
    "Frames:    [====] [====] [====] [====]  \n",
    "\n",
    "### Windowing\n",
    "\n",
    "when we split the audio into frames the edges of the frames are abruptly cut off. This leads to a problem called **Spectral Leakage**.  \n",
    "\n",
    "Now there are two parts to it, we visualize things in the time domain whereas the computations happen in the frequency domain.  \n",
    "\n",
    "1. The Intuition (Time Domain): You are looking at how frames sit next to each other in a sequence. If the end of Frame A doesn't match the start of Frame B, you get a physical \"click\" or \"pop\" because the speaker diaphragm has to teleport instantly from one position to another.  \n",
    "\n",
    "2. The Math (Frequency Domain): The Fast Fourier Transform (FFT)—the math used to see frequencies—doesn't know about \"other\" frames. It only looks at one frame at a time. To do its job, it mathematically \"wraps\" that frame around a cylinder so the end touches the beginning.  \n",
    "\n",
    "By applying a window (like a Hamming window), you force both the beginning and the end of the frame to zero.\n",
    "\n",
    "Now, no matter what the next frame looks like, or how you \"wrap\" the signal into a circle, the transition is always 0 to 0. The \"jump\" is gone, and the artifacts disappear.Therefore no spectral leakage anymore.  \n",
    "\n",
    "\n",
    "Raw frame:     |████████████|  \n",
    "Windowed:       ▁▂▅████▅▂▁  \n",
    "\n",
    "Purpose:  \n",
    "Reduce edge artifacts  \n",
    "Make frequency analysis cleaner  \n",
    "\n",
    "- You usually never implement this manually — libraries do it.\n",
    "\n",
    "## Time Domain vs Frequency domain\n",
    "\n",
    "### Time Domain (What we started with)\n",
    "\n",
    "Waveform is in time domain. It is a graph of amplitude over time.  \n",
    "It captures energy and temporal patterns.  \n",
    "Examples - Loudness, Silence, Speaking rate, etc.\n",
    "\n",
    "### Frequency domain (how humans hear)\n",
    "\n",
    "Apply FFT per frame:  \n",
    "\n",
    "Waveform → Frames → FFT → Spectrum  \n",
    "\n",
    "![flowchart](images/signalspectrogram.png)\n",
    "\n",
    "The above image is an example where a signal is converted from a time domain to a frequency domain.  \n",
    "In the spectrogram the darkness (in this case ) represents the magnitude or loudness of that frequency, and white places represent the absence of that frequency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechbrainDemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
